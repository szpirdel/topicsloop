# ğŸ¤– TopicsLoop AI Architecture

**Last Updated:** 2025-11-17
**Current Version:** 0.2.0
**Status:** In Production (Local Development)

---

## ğŸ“‹ Table of Contents

1. [Executive Summary](#executive-summary)
2. [AI Pipeline Overview](#ai-pipeline-overview)
3. [Technology Stack](#technology-stack)
4. [Detailed Pipeline](#detailed-pipeline)
5. [Fallback Mechanisms](#fallback-mechanisms)
6. [Performance & Configuration](#performance--configuration)
7. [GNN Status & Future](#gnn-status--future)
8. [API Endpoints](#api-endpoints)
9. [Common Questions](#common-questions)

---

## 1. Executive Summary

### What Does Our AI Do?

TopicsLoop uses **semantic AI** to understand content meaning (not just keywords) and find connections between posts, categories, and users.

**Key Capabilities:**
- ğŸ“ **Understand post content** - Convert text to 384-dimensional semantic vectors
- ğŸ”— **Find similar posts** - Calculate similarity using cosine distance
- ğŸ¯ **Auto-categorize** - Suggest categories based on content
- ğŸ‘¤ **Personalize recommendations** - Match users with relevant content

### Technology at a Glance

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Embeddings** | Sentence-Transformers (`all-MiniLM-L6-v2`) | Convert text â†’ semantic vectors |
| **Similarity** | Cosine similarity | Measure how similar two texts are |
| **Caching** | Django Cache (1 hour TTL) | Speed up repeated queries |
| **Fallback** | TF-IDF (scikit-learn) | Work when sentence-transformers unavailable |
| **GNN (Future)** | PyTorch Geometric | Graph-based reasoning (not yet active) |

---

## 2. AI Pipeline Overview

### The Complete Flow: Text â†’ Recommendations

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER CREATES POST                           â”‚
â”‚                 Title: "Machine Learning Basics"                    â”‚
â”‚                 Content: "Introduction to neural networks..."       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STEP 1: TEXT PREPROCESSING                       â”‚
â”‚  â€¢ Combine: title + content + category + tags                      â”‚
â”‚  â€¢ Limit content to first 200 words                                â”‚
â”‚  â€¢ Example combined text:                                          â”‚
â”‚    "Machine Learning Basics Introduction to neural networks...     â”‚
â”‚     Category: Technology > AI > Machine Learning Tags: ML, AI"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STEP 2: GENERATE EMBEDDING (384 dimensions)            â”‚
â”‚                                                                     â”‚
â”‚  METHOD 1: Sentence-Transformers (PRIMARY)                         â”‚
â”‚  âœ“ Model: all-MiniLM-L6-v2                                        â”‚
â”‚  âœ“ Input: Combined text                                           â”‚
â”‚  âœ“ Output: [0.12, -0.34, 0.56, ..., 0.89] (384 floats)          â”‚
â”‚  âœ“ Speed: ~50ms per post                                          â”‚
â”‚  âœ“ Cache: Yes (Django cache, 1 hour)                              â”‚
â”‚                                                                     â”‚
â”‚  METHOD 2: TF-IDF Fallback (BACKUP)                               â”‚
â”‚  âš  Used when: sentence-transformers not available                 â”‚
â”‚  â€¢ Scikit-learn TfidfVectorizer                                   â”‚
â”‚  â€¢ Max 384 features (to match MiniLM dimension)                   â”‚
â”‚  â€¢ Less accurate, but works offline                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   STEP 3: STORE IN DATABASE                         â”‚
â”‚  â€¢ Table: ai_models_postembedding                                  â”‚
â”‚  â€¢ Columns:                                                        â”‚
â”‚    - post_id (foreign key)                                         â”‚
â”‚    - embedding (JSONField - array of 384 floats)                  â”‚
â”‚    - model_name ('all-MiniLM-L6-v2')                              â”‚
â”‚    - created_at                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STEP 4: FIND SIMILAR POSTS (On Request)                â”‚
â”‚                                                                     â”‚
â”‚  When user views a post, we:                                       â”‚
â”‚  1. Get target post embedding: E_target                            â”‚
â”‚  2. Get ALL other post embeddings: E_1, E_2, ..., E_n             â”‚
â”‚  3. Calculate cosine similarity for each:                          â”‚
â”‚                                                                     â”‚
â”‚     similarity(E_target, E_i) = (E_target Â· E_i) /                â”‚
â”‚                                  (||E_target|| * ||E_i||)          â”‚
â”‚                                                                     â”‚
â”‚     Range: -1 to +1                                                â”‚
â”‚     â€¢ +1 = identical meaning                                       â”‚
â”‚     â€¢ 0  = unrelated                                               â”‚
â”‚     â€¢ -1 = opposite meaning                                        â”‚
â”‚                                                                     â”‚
â”‚  4. Filter: Keep only similarity >= threshold (default 0.7)        â”‚
â”‚  5. Sort: Highest similarity first                                 â”‚
â”‚  6. Limit: Return top 10                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STEP 5: RETURN RESULTS                           â”‚
â”‚  [                                                                  â”‚
â”‚    {                                                                â”‚
â”‚      "post": {...},                                                 â”‚
â”‚      "similarity_score": 0.87,                                      â”‚
â”‚      "method_used": "semantic_embeddings"                           â”‚
â”‚    },                                                               â”‚
â”‚    {                                                                â”‚
â”‚      "post": {...},                                                 â”‚
â”‚      "similarity_score": 0.75,                                      â”‚
â”‚      "method_used": "semantic_embeddings"                           â”‚
â”‚    }                                                                â”‚
â”‚  ]                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Technology Stack

### ğŸ§  Primary: Sentence-Transformers

**Model:** `all-MiniLM-L6-v2`

**Why This Model?**
| Criterion | MiniLM-L6-v2 | Alternatives |
|-----------|--------------|--------------|
| **Speed** | â­â­â­â­â­ Very fast (~50ms) | mpnet: â­â­â­ Slower (~150ms) |
| **Accuracy** | â­â­â­â­ Good enough for MVP | mpnet: â­â­â­â­â­ Better |
| **Dimensions** | 384 (compact) | mpnet: 768 (larger) |
| **Size** | 80 MB | mpnet: 420 MB |
| **Languages** | English only | multilingual: Multiple |

**Decision:** MiniLM is perfect for MVP - fast, small, accurate enough.

**Source Code:** [gnn_models/embeddings.py](../gnn_models/embeddings.py)

---

### ğŸ”¢ Similarity Calculation: Cosine Similarity

**What is it?**
Cosine similarity measures the angle between two vectors. It tells us how "similar" two pieces of text are in meaning.

**Formula:**
```
cosine_similarity(A, B) = (A Â· B) / (||A|| * ||B||)

Where:
â€¢ A Â· B = dot product (sum of element-wise multiplication)
â€¢ ||A|| = magnitude of vector A (sqrt of sum of squares)
â€¢ ||B|| = magnitude of vector B
```

**Example:**
```python
# Two embeddings
E1 = [0.5, 0.3, 0.8]  # Post about "machine learning"
E2 = [0.6, 0.2, 0.9]  # Post about "deep learning"

# Dot product
dot = (0.5*0.6) + (0.3*0.2) + (0.8*0.9) = 1.08

# Magnitudes
norm_E1 = sqrt(0.5Â² + 0.3Â² + 0.8Â²) = 0.989
norm_E2 = sqrt(0.6Â² + 0.2Â² + 0.9Â²) = 1.106

# Cosine similarity
similarity = 1.08 / (0.989 * 1.106) = 0.987

Result: 0.987 (very similar! They're both about ML)
```

**Code Location:** [gnn_models/embeddings.py:260-293](../gnn_models/embeddings.py#L260-L293)

---

### ğŸ”„ Batch Processing

**What:** Process multiple texts at once instead of one-by-one.

**Default Batch Size:** 32

**Why Batching?**
- âš¡ **Faster:** GPU/CPU can parallelize computations
- ğŸ’¾ **Memory efficient:** Load model once, process many items
- ğŸ¯ **Better throughput:** 32 texts in ~200ms vs. 1 text in ~50ms (6x faster!)

**Example:**
```python
# BAD: One by one (slow)
for post in posts:  # 100 posts
    embedding = model.encode(post.text)  # 50ms each = 5 seconds total

# GOOD: Batch (fast)
texts = [post.text for post in posts]
embeddings = model.encode(texts, batch_size=32)  # ~600ms total (8x faster!)
```

**Code Location:** [gnn_models/embeddings.py:66-131](../gnn_models/embeddings.py#L66-L131)

---

## 4. Detailed Pipeline

### 4.1. Text Preprocessing

**Location:** [gnn_models/embeddings.py:237-258](../gnn_models/embeddings.py#L237-L258)

**What happens:**
```python
def _combine_post_text(title, content, category, tags, category_path):
    combined = []

    # 1. Add title
    if title:
        combined.append(title)  # "Machine Learning Basics"

    # 2. Add content (max 200 words to avoid too long sequences)
    if content:
        content_words = content.split()[:200]
        combined.append(" ".join(content_words))

    # 3. Add hierarchical category path (important for context!)
    if category_path:
        combined.append(f"Category: {category_path}")
        # Example: "Category: Technology > AI > Machine Learning"

    # 4. Add tags
    if tags:
        combined.append(f"Tags: {', '.join(tags)}")
        # Example: "Tags: ML, neural-networks, AI"

    return " ".join(combined)
```

**Why limit to 200 words?**
- Transformer models have max token limits (512 tokens for MiniLM)
- Longer text = slower processing
- Most semantic meaning is in first paragraphs anyway

---

### 4.2. Embedding Generation

**Location:** [gnn_models/embeddings.py:66-131](../gnn_models/embeddings.py#L66-L131)

**Flow:**
```python
def encode_texts(texts, batch_size=32):
    # 1. Check cache first (avoid recomputing!)
    cache_keys = [f"embedding_{hash(text)}_{model_name}" for text in texts]
    cached = [cache.get(key) for key in cache_keys]

    # 2. Identify which texts need computation
    uncached_texts = [text for text, cached in zip(texts, cached) if not cached]

    # 3. Generate embeddings only for uncached (SAVES TIME!)
    if uncached_texts:
        new_embeddings = model.encode(
            uncached_texts,
            batch_size=32,  # Process 32 at once
            show_progress_bar=len(uncached_texts) > 10  # Show progress for large batches
        )

        # 4. Cache results for 1 hour (3600 seconds)
        for text, embedding, cache_key in zip(uncached_texts, new_embeddings, cache_keys):
            cache.set(cache_key, embedding.tolist(), 3600)

    # 5. Combine cached + newly computed embeddings
    return combined_embeddings
```

**Cache Hit Rate:**
- First request: 0% (must compute all)
- Subsequent requests (within 1h): ~90% (only new posts computed)
- Performance gain: **10-50x faster** for cached embeddings!

---

### 4.3. Similarity Calculation

**Location:** [api/views.py:565-595](../api/views.py#L565-L595)

**API Endpoint:** `GET /api/posts/{post_id}/similar/`

**Query Parameters:**
| Parameter | Default | Description |
|-----------|---------|-------------|
| `threshold` | 0.7 | Minimum similarity score (0.0-1.0) |
| `limit` | 10 | Max number of results |
| `method` | `fallback` | `gnn` for semantic, `fallback` for traditional |

**Example Request:**
```bash
GET /api/posts/42/similar/?threshold=0.6&limit=5&method=gnn
```

**Response:**
```json
{
  "target_post": {
    "id": 42,
    "title": "Introduction to Neural Networks",
    "content": "..."
  },
  "similar_posts": [
    {
      "post": {
        "id": 58,
        "title": "Deep Learning Fundamentals"
      },
      "similarity_score": 0.87,
      "method_used": "semantic_embeddings"
    },
    {
      "post": {
        "id": 91,
        "title": "Backpropagation Explained"
      },
      "similarity_score": 0.75,
      "method_used": "semantic_embeddings"
    }
  ],
  "search_params": {
    "threshold": 0.6,
    "limit": 5,
    "method": "gnn"
  }
}
```

---

## 5. Fallback Mechanisms

### When Do Fallbacks Trigger?

Our system has **3 layers of fallback** to ensure it always works:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 1: Sentence-Transformers (PRIMARY)                   â”‚
â”‚  âœ“ all-MiniLM-L6-v2 model                                  â”‚
â”‚  âœ“ Best quality                                            â”‚
â”‚  âœ“ Fast (50ms per text)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ âŒ If model fails to load
                   â”‚ âŒ If PyTorch not available
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 2: TF-IDF (FALLBACK)                                â”‚
â”‚  âš  Scikit-learn TfidfVectorizer                           â”‚
â”‚  âš  Lower quality (keyword-based, not semantic)            â”‚
â”‚  âœ“ Still useful for finding similar topics                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ âŒ If scikit-learn fails
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 3: Random Embeddings (LAST RESORT)                  â”‚
â”‚  âš  Random normal distribution (0, 0.1)                     â”‚
â”‚  âš  No real semantic meaning                                â”‚
â”‚  âœ“ System doesn't crash, just returns random results       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How to Tell Which Method Was Used?

**Check API response:**
```json
{
  "method_used": "semantic_embeddings",  // âœ“ Using sentence-transformers
  "method_used": "traditional_fallback",  // âš  Using TF-IDF
}
```

**Check logs:**
```
INFO: Sentence Transformers available - full embedding functionality enabled
INFO: Model loaded successfully. Embedding dimension: 384

# OR

WARNING: Sentence Transformers not available - using fallback methods
WARNING: Generating fallback TF-IDF embeddings
```

### TF-IDF Fallback Details

**Location:** [gnn_models/embeddings.py:132-158](../gnn_models/embeddings.py#L132-L158)

**How it works:**
```python
def _fallback_embeddings(texts):
    # TF-IDF: Term Frequency - Inverse Document Frequency
    # Counts word importance based on:
    # - How often word appears in THIS document (TF)
    # - How rare word is across ALL documents (IDF)

    vectorizer = TfidfVectorizer(
        max_features=384,        # Match MiniLM dimension
        stop_words='english',    # Remove "the", "a", "is", etc.
        ngram_range=(1, 2)       # Use 1-word and 2-word phrases
    )

    embeddings = vectorizer.fit_transform(texts).toarray()
    return embeddings  # Shape: [num_texts, 384]
```

**TF-IDF vs. Sentence-Transformers:**

| Aspect | Sentence-Transformers | TF-IDF |
|--------|---------------------|---------|
| **Understanding** | Semantic (meaning) | Keyword (word counts) |
| **Example 1** | "car" and "automobile" = **similar** | **Different** (different words!) |
| **Example 2** | "I love Python" and "Python is great" = **similar** | Somewhat similar |
| **Quality** | â­â­â­â­â­ | â­â­â­ |
| **Speed** | â­â­â­â­ | â­â­â­â­â­ |
| **Dependencies** | PyTorch, transformers | Only scikit-learn |

---

## 6. Performance & Configuration

### Current Configuration

**Model Settings:**
```python
# gnn_models/embeddings.py
MODEL_NAME = 'all-MiniLM-L6-v2'
EMBEDDING_DIM = 384
BATCH_SIZE = 32
```

**Cache Settings:**
```python
# Cache timeout: 1 hour (3600 seconds)
cache.set(cache_key, embedding.tolist(), 3600)
```

**Similarity Thresholds:**
| Endpoint | Default Threshold | Meaning |
|----------|-------------------|---------|
| Similar Posts | 0.7 | High similarity required |
| Similar Categories | 0.7 | High similarity required |
| Recommendations | 0.4 | Lower threshold (find more options) |
| Category Network | 0.6 | Moderate similarity |

**Why different thresholds?**
- **Similar Posts (0.7):** User expects very relevant results
- **Recommendations (0.4):** Discovery - show broader connections
- **Category Network (0.6):** Balance between connections and clutter

### Performance Benchmarks

**TODO:** We need to measure this! (See backlog item 4.5 Phase 1)

**What we should measure:**
```python
# Time to embed 100 posts
# Expected: ~2-5 seconds (with caching: <100ms)

# Time to find 10 similar posts
# Expected: ~50-200ms

# Memory usage during inference
# Expected: ~500MB (model loaded)

# Cache hit rate
# Expected: >80% after warmup
```

---

## 7. GNN Status & Future

### Current Status: ğŸŸ¡ Code Exists, Not Active

**What we have:**
- âœ… GNN model architecture defined ([gnn_models/models.py](../gnn_models/models.py))
- âœ… Graph building logic ([gnn_models/integration.py](../gnn_models/integration.py))
- âœ… Training pipeline skeleton ([gnn_models/training.py](../gnn_models/training.py))

**What we DON'T have:**
- âŒ Trained GNN model (no checkpoint file)
- âŒ Active usage in production pipeline
- âŒ Benchmarks comparing GNN vs. sentence-transformers only

### When Will GNN Be Active?

**According to backlog (Section 4.5):**

**Phase 3 (v0.4.0)** - Future enhancement
- Train GNN on real TopicsLoop data
- Integrate into inference pipeline
- A/B test GNN vs. sentence-transformers
- Decide if quality improvement justifies complexity

**Why not now?**
- GNN requires training data (we're about to import 200 real articles!)
- Adds complexity (debugging, maintenance)
- Sentence-transformers work well enough for MVP
- Better to validate with users first

### How Would GNN Improve Things?

**GNN Advantage:** Graph-aware reasoning

**Example:**
```
Sentence-Transformers: Looks at EACH post independently
  Post A: "Neural networks for image classification"
  Post B: "Deep learning in computer vision"
  â†’ Similarity: 0.75 (good!)

GNN: Looks at post + its CONNECTIONS
  Post A is in:
    - Category: ML > Computer Vision
    - Connected to: Post C (CNNs), Post D (ResNet)
    - User interactions: 10 likes, 3 comments

  Post B is in:
    - Category: ML > Computer Vision
    - Connected to: Post E (Transfer learning), Post A
    - User interactions: 15 likes, 8 comments

  â†’ GNN can reason: "Both in same category, shared connections,
                     similar engagement â†’ probably VERY similar"
  â†’ Improved similarity: 0.92 (even better!)
```

**Trade-off:**
- âœ… Better quality (graph context)
- âŒ More complex (training, maintenance)
- âŒ Slower inference (GNN forward pass)

---

## 8. API Endpoints

### Summary Table

| Endpoint | Method | Purpose | Threshold |
|----------|--------|---------|-----------|
| `/api/posts/{id}/similar/` | GET | Find similar posts | 0.7 |
| `/api/categories/{id}/similar/` | GET | Find similar categories | 0.7 |
| `/api/ai/recommendations/` | GET | Personalized recommendations | 0.4 |
| `/api/ai/auto-categorize/` | POST | Suggest categories for text | N/A |
| `/api/ai/embeddings/stats/` | GET | AI system statistics | N/A |
| `/api/ai/embeddings/` | POST | Generate embeddings manually | N/A |
| `/api/visualization/category-network/` | GET | Category similarity graph | 0.6 |

### Detailed Examples

#### 1. Find Similar Posts

```bash
GET /api/posts/42/similar/?threshold=0.6&limit=10&method=gnn
```

**Response:**
```json
{
  "target_post": {...},
  "similar_posts": [
    {
      "post": {...},
      "similarity_score": 0.87,
      "method_used": "semantic_embeddings"
    }
  ]
}
```

#### 2. Auto-Categorize

```bash
POST /api/ai/auto-categorize/
{
  "title": "Introduction to React Hooks",
  "content": "React Hooks are functions that let you use state..."
}
```

**Response:**
```json
{
  "suggested_categories": [
    {
      "category": "Technology",
      "confidence": 0.92
    },
    {
      "category": "Web Development",
      "confidence": 0.85
    }
  ]
}
```

#### 3. Embeddings Statistics

```bash
GET /api/ai/embeddings/stats/
```

**Response:**
```json
{
  "post_embeddings": {
    "total_embeddings": 109,
    "unique_models": 1,
    "models": ["all-MiniLM-L6-v2"]
  },
  "similarities": {
    "total_pairs": 450,
    "avg_similarity": 0.62
  }
}
```

---

## 9. Common Questions

### Q: Why aren't embeddings cached in the DATABASE?

**A:** Currently we use Django's default cache (often in-memory or Redis) with 1-hour TTL.

**Backlog item 4.5 Phase 1** proposes adding database caching:
- Add `embedding` field to `Post` model (JSONField)
- Generate embeddings on post create/update
- Serve from DB (persistent, no TTL)

**Why not done yet?**
- MVP works fine with current cache
- DB cache means migrations, testing
- Planned for v0.3.0

### Q: Can I change the similarity threshold?

**A:** Yes! Pass `?threshold=X` to API endpoints.

**Recommended values:**
- **0.8-1.0:** Very strict (almost identical content)
- **0.6-0.8:** Moderate (clearly related topics)  â¬…ï¸ **DEFAULT**
- **0.4-0.6:** Loose (discover broader connections)
- **< 0.4:** Very loose (may include tangentially related)

### Q: How do I know if sentence-transformers is working?

**A:** Check logs when backend starts:

```bash
docker-compose logs web | grep -i "sentence"
```

**Good output:**
```
INFO: Sentence Transformers available - full embedding functionality enabled
INFO: Model loaded successfully. Embedding dimension: 384
```

**Bad output (fallback active):**
```
WARNING: Sentence Transformers not available - using fallback methods
```

### Q: What happens if I have 10,000 posts?

**A:** Current approach (load ALL embeddings, compare) will be slow.

**Solutions (future work):**
1. **Vector database** (Pinecone, Weaviate, Milvus)
   - Optimized for similarity search
   - Sub-millisecond queries even with millions of vectors

2. **Approximate Nearest Neighbors** (ANN)
   - FAISS, Annoy, HNSW
   - Trade tiny accuracy loss for 100x speed

3. **Database indexing**
   - PostgreSQL pgvector extension
   - Native vector similarity in Postgres

**For now (< 1000 posts):** Current approach is fine.

---

## ğŸ“ˆ Next Steps (From Backlog)

### Phase 1: Performance Audit (P0 - MUST DO)
- [ ] Benchmark current embedding performance
- [ ] Implement DB caching for embeddings
- [ ] Tune similarity thresholds based on real data
- [ ] Document performance metrics

### Phase 2: Model Optimization (P1 - Should Do)
- [ ] Compare MiniLM vs. mpnet vs. multilingual
- [ ] Implement batch embedding generation
- [ ] Profile and optimize inference
- [ ] Choose optimal model for production

### Phase 3: GNN Integration (P2 - Nice to Have)
- [ ] Train GNN on real TopicsLoop data
- [ ] Integrate GNN into inference pipeline
- [ ] A/B test quality improvement
- [ ] Decide on production usage

---

## ğŸ”— References

**Code Locations:**
- Embeddings: [gnn_models/embeddings.py](../gnn_models/embeddings.py)
- API Views: [api/views.py](../api/views.py)
- Models: [ai_models/models.py](../ai_models/models.py), [gnn_models/models.py](../gnn_models/models.py)

**External Documentation:**
- [Sentence-Transformers](https://www.sbert.net/)
- [All-MiniLM-L6-v2 Model Card](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
- [Cosine Similarity Explained](https://en.wikipedia.org/wiki/Cosine_similarity)
- [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/)

---

**Document Status:** âœ… Complete
**Maintained By:** Development Team
**Last Review:** 2025-11-17

**Questions?** Check [project_backlog.md](../project_backlog.md) Section 4.5 for AI/ML roadmap.
